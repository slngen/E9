{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ProcessLine.S1 import S1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_id2str = {1:\"01F\",2:\"01M\",3:\"02F\",4:\"02M\",5:\"03F\",6:\"03M\",7:\"04F\",8:\"04M\",9:\"05F\",10:\"05M\"}\n",
    "part_id = 5\n",
    "mode = part_id2str[part_id]+\"/S1\"\n",
    "# training\n",
    "epochs = 100\n",
    "lr = 0.000002\n",
    "accumulation_steps = 8\n",
    "warm_up = 10\n",
    "# whether to save model\n",
    "save_steps_flag = False  # save model per \"save_steps\" steps\n",
    "save_steps = 10\n",
    "save_best_flag = False  # save the best effect model\n",
    "# checkpoint\n",
    "checkpoint_epoch = 0\n",
    "checkpoint_path = \"\"\n",
    "time_stamp = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name())\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.s1 = S1()\n",
    "\n",
    "    def forward(self, speech):\n",
    "        S1_logits = self.s1(speech)\n",
    "        logits = {\"s1\": S1_logits}\n",
    "        return logits\n",
    "\n",
    "    def loss(self, logits, label):\n",
    "        loss = self.s1.loss(logits[\"s1\"], label)\n",
    "        return loss\n",
    "\n",
    "model = Model()\n",
    "model.to(device)\n",
    "# checkpoint\n",
    "if checkpoint_epoch>0:\n",
    "    model= torch.load(checkpoint_path)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label dict\n",
    "label2id = {'e0': 0, 'e1': 1, 'e2': 2, 'e3': 3}\n",
    "\n",
    "# Dataset\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, mode):\n",
    "        if mode == \"train\":\n",
    "            df_data = pd.read_csv(\"./Data/iemocap/iemocap_\"+part_id2str[part_id]+\".train.csv\")\n",
    "        else:\n",
    "            df_data = pd.read_csv(\"./Data/iemocap/iemocap_\"+part_id2str[part_id]+\".test.csv\")\n",
    "\n",
    "        self.speech = []\n",
    "        self.label = []\n",
    "        self.index = []\n",
    "\n",
    "        for _rows in tqdm(df_data.iterrows()):\n",
    "            _label = _rows[1][\"emotion\"]\n",
    "            _path = _rows[1][\"file\"]\n",
    "            _index = _rows[0]\n",
    "            _, wave = wavfile.read(_path)\n",
    "            self.speech.append(wave)\n",
    "            self.label.append(torch.tensor(label2id[_label]))\n",
    "            self.index.append(_index)\n",
    "        self.len = len(self.label)\n",
    "        print(\"Load <\", mode,\"> data successfully! \\n\\tTotal \"+str(self.len)+\" samples.\")\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.index[index], self.speech[index], self.label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "train_dataset = dataset(\"train\")\n",
    "dev_dataset = dataset(\"dev\")\n",
    "\n",
    "# Dataloader\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=1, num_workers=4, shuffle=True)\n",
    "dev_dataloader = DataLoader(dataset=dev_dataset, batch_size=1, num_workers=4, shuffle=True)\n",
    "len_train_dataloder = len(train_dataloader)\n",
    "len_dev_dataloader = len(dev_dataloader)\n",
    "print(\"Make dataloder successfully! \\n\\ttrain:\",len_train_dataloder,\"\\n\\tdev:\",len_dev_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time stamp\n",
    "if checkpoint_epoch==0:\n",
    "    time_stamp = time.strftime(\"%Y-%m-%d_%H-%M\", time.localtime())\n",
    "# make dir\n",
    "if save_steps_flag or save_best_flag:\n",
    "    if not os.path.exists(\"./Models/\"+mode+\"/\"):\n",
    "        os.makedirs(\"./Models/\"+mode+\"/\")\n",
    "if not os.path.exists(\"./Logs/\"+mode+\"/\"):\n",
    "    os.makedirs(\"./Logs/\"+mode+\"/\")\n",
    "if not os.path.exists(\"./Logs/\"+mode+\"/Logits/\"+time_stamp+\"/\"):\n",
    "    os.makedirs(\"./Logs/\"+mode+\"/Logits/\"+time_stamp+\"/\")\n",
    "# make log file\n",
    "train_log = open(\"./Logs/\"+mode+\"/\"+time_stamp+\"_train.txt\",\"a+\")\n",
    "dev_log = open(\"./Logs/\"+mode+\"/\"+time_stamp+\"_dev.txt\",\"a+\")\n",
    "train_acc_log = open(\"./Logs/\"+mode+\"/\"+time_stamp+\"_train_acc.txt\",\"a+\")\n",
    "dev_acc_log = open(\"./Logs/\"+mode+\"/\"+time_stamp+\"_dev_acc.txt\",\"a+\")\n",
    "lr_log = open(\"./Logs/\"+mode+\"/\"+time_stamp+\"_lr.txt\",\"a+\")\n",
    "lr_log.write(\"lr=\"+str(lr)+\",accumulation_steps=\"+str(accumulation_steps)+\"\\n\")\n",
    "lr_log.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW([{'params': model.parameters(), 'lr': lr}])\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: \n",
    "                math.pow(512, 0.5)*min(math.pow(epoch+1+checkpoint_epoch,-0.5), (epoch+1+checkpoint_epoch)*math.pow(warm_up,-1.5))\n",
    "        )\n",
    "# Train!\n",
    "steps = 0\n",
    "loss_item = 0\n",
    "max_dev_acc = 0\n",
    "for epoch in tqdm(range(1+checkpoint_epoch,epochs+checkpoint_epoch+1)):\n",
    "    logits_record = {}  # save single epoch logits\n",
    "    train_acc_num = 0\n",
    "    model.train()\n",
    "    for index,(data_index, speech, label) in enumerate(train_dataloader):\n",
    "        steps += 1\n",
    "        label = label.to(device)\n",
    "        logits = model(speech)\n",
    "        output = torch.argmax(logits[\"s1\"], dim=-1)\n",
    "        train_acc_num += (output == label).sum().item()\n",
    "        loss_total = model.loss(logits, label)\n",
    "        loss_item += loss_total.cpu().item()\n",
    "        loss_total = loss_total/accumulation_steps        \n",
    "        loss_total.backward()\n",
    "        if steps%accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            print((index+1)//accumulation_steps,\"/\",\n",
    "                    len_train_dataloder//accumulation_steps,\n",
    "                    \"-->\",np.around(loss_item/accumulation_steps,4)\n",
    "                    )\n",
    "            # logging\n",
    "            train_log.write(str(loss_item/accumulation_steps)+\"\\n\")\n",
    "            train_log.flush()\n",
    "            # reset\n",
    "            optimizer.zero_grad()\n",
    "            steps = 0\n",
    "            loss_item = 0\n",
    "    # logging\n",
    "    train_acc = 100*train_acc_num / len(train_dataset)\n",
    "    train_acc_log.write(str(train_acc)+\"\\n\")\n",
    "    train_acc_log.flush()\n",
    "    # update learning rate\n",
    "    scheduler.step() \n",
    "\n",
    "    ## dev\n",
    "    model.eval()\n",
    "    dev_loss = 0\n",
    "    dev_index = 0\n",
    "    dev_acc_num = 0\n",
    "    with torch.no_grad():\n",
    "        for index,(data_index, speech, label) in enumerate(dev_dataloader):\n",
    "            label = label.to(device)\n",
    "            logits = model(speech)\n",
    "            output = torch.argmax(logits[\"s1\"], dim=-1)\n",
    "            dev_acc_num += (output == label).sum().item()\n",
    "            loss = model.loss(logits, label)\n",
    "            dev_loss += loss.cpu().item()\n",
    "            dev_index += 1\n",
    "            logits_record[data_index.item()] = logits[\"s1\"].detach().cpu().tolist()\n",
    "    dev_loss /= dev_index\n",
    "    dev_acc = 100 * dev_acc_num / len(dev_dataset)\n",
    "    # logging\n",
    "    df_logits_record = pd.DataFrame.from_dict(logits_record, orient=\"index\")\n",
    "    df_logits_record.columns = [\"logits\"]\n",
    "    df_logits_record.index.name = \"index\"\n",
    "    df_logits_record.to_csv(\"./Logs/\"+mode+\"/Logits/\"+time_stamp+\"/\"+str(epoch)+\".csv\")\n",
    "    dev_log.write(str(dev_loss)+\"\\n\")\n",
    "    dev_log.flush()\n",
    "    dev_acc_log.write(str(dev_acc)+\"\\n\")\n",
    "    dev_acc_log.flush()\n",
    "    lr_log.write(str(optimizer.state_dict()['param_groups'][0]['lr'])+\"\\n\")\n",
    "    lr_log.flush()\n",
    "    print(\"Epoch:\",epoch,\n",
    "            \"\\tDev_Loss:\",round(dev_loss, 3),\n",
    "            \"\\tDev_acc:\",round(dev_acc, 2),\n",
    "            \"%\\tTrain_acc:\",round(train_acc, 2),\n",
    "            \"%\\tAcc_num:\",dev_acc_num)\n",
    "    if save_steps_flag and epoch%save_steps == 0:\n",
    "        torch.save(model,\"./Models/\"+mode+\"/\"+time_stamp\n",
    "                +\"_Epoch\"+str(epoch)\n",
    "                +\"_Lr\"+str(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "                +\"_DevLoss\"+str(np.around(dev_loss,3))\n",
    "                +\"_DevAcc\"+str(np.around(dev_acc,3))\n",
    "                +\".pt\")\n",
    "    if save_best_flag and epoch>epochs/2 and max_dev_acc<dev_acc:\n",
    "        max_dev_acc = dev_acc\n",
    "        torch.save(model,\"./Models/\"+mode+\"/\"+time_stamp\n",
    "                +\"_Epoch\"+str(epoch)\n",
    "                +\"_Lr\"+str(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "                +\"_DevLoss\"+str(np.around(dev_loss,3))\n",
    "                +\"_DevAcc\"+str(np.around(dev_acc,3))\n",
    "                +\".pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_acc_log.seek(0,0)\n",
    "train_acc_log.seek(0,0)\n",
    "df_dev_acc = pd.read_csv(dev_acc_log)\n",
    "df_train_acc = pd.read_csv(train_acc_log)\n",
    "df_dev_acc.columns = [\"dev acc\"]\n",
    "df_train_acc.columns = [\"train acc\"]\n",
    "ax = df_dev_acc.plot()\n",
    "df_train_acc.plot(ax=ax)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"acc\")\n",
    "plt.savefig(\"./Logs/\"+mode+\"/\"+time_stamp+\".png\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f18d1d36c0fabcb8b22250053d88db89962fe129b2054c2ee331cf33d9695e3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
